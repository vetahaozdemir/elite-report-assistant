"""
PDF dosyalarÄ±nÄ± indeksleme ve vektÃ¶r veritabanÄ±na ekleme modÃ¼lÃ¼
"""

import os
import uuid
from pathlib import Path
from typing import List, Dict
from .pdf_processor import PDFProcessor
from .vector_db import VectorDatabase

class DocumentIndexer:
    """PDF dosyalarÄ±nÄ± iÅŸleyip vektÃ¶r veritabanÄ±na ekleyen sÄ±nÄ±f"""
    
    def __init__(self, vector_db: VectorDatabase = None, api_key: str = None):
        """
        DocumentIndexer initialization
        
        Args:
            vector_db (VectorDatabase): Mevcut VectorDatabase instance
            api_key (str): Gemini API key
        """
        self.pdf_processor = PDFProcessor()
        self.vector_db = vector_db or VectorDatabase(api_key=api_key)
    
    def index_single_file(self, file_path: str, chunk_size: int = 1000, overlap: int = 200) -> int:
        """
        Tek bir PDF dosyasÄ±nÄ± indeksle
        
        Args:
            file_path (str): PDF dosya yolu
            chunk_size (int): Chunk boyutu
            overlap (int): Ã–rtÃ¼ÅŸme miktarÄ±
            
        Returns:
            int: Eklenen chunk sayÄ±sÄ±
        """
        try:
            # PDF'i iÅŸle
            processed_data = self.pdf_processor.process_pdf_file(file_path, chunk_size, overlap)
            
            # Her chunk iÃ§in dokuman oluÅŸtur
            documents = []
            base_filename = Path(file_path).stem
            
            for i, chunk in enumerate(processed_data['chunks']):
                doc_id = f"{base_filename}_chunk_{i}_{uuid.uuid4().hex[:8]}"
                
                metadata = {
                    'source_file': processed_data['metadata']['filename'],
                    'file_path': file_path,
                    'chunk_index': i,
                    'total_chunks': processed_data['metadata']['total_chunks'],
                    'file_size': processed_data['metadata']['file_size'],
                    'document_type': 'social_service_report'
                }
                
                documents.append({
                    'id': doc_id,
                    'text': chunk,
                    'metadata': metadata
                })
            
            # VeritabanÄ±na ekle
            added_count = self.vector_db.add_multiple_documents(documents)
            
            print(f"âœ… {file_path} dosyasÄ±ndan {added_count}/{len(documents)} chunk eklendi")
            return added_count
            
        except Exception as e:
            print(f"âŒ {file_path} dosyasÄ± indekslenirken hata: {e}")
            return 0
    
    def index_directory(self, directory_path: str, chunk_size: int = 1000, overlap: int = 200) -> Dict:
        """
        Bir klasÃ¶rdeki tÃ¼m PDF dosyalarÄ±nÄ± indeksle
        
        Args:
            directory_path (str): KlasÃ¶r yolu
            chunk_size (int): Chunk boyutu
            overlap (int): Ã–rtÃ¼ÅŸme miktarÄ±
            
        Returns:
            Dict: Ä°ndeksleme sonuÃ§ raporu
        """
        directory = Path(directory_path)
        
        if not directory.exists():
            raise FileNotFoundError(f"KlasÃ¶r bulunamadÄ±: {directory_path}")
        
        pdf_files = list(directory.glob("*.pdf"))
        
        if not pdf_files:
            print(f"âš ï¸  {directory_path} klasÃ¶rÃ¼nde PDF dosyasÄ± bulunamadÄ±")
            return {
                'total_files': 0,
                'processed_files': 0,
                'total_chunks': 0,
                'failed_files': []
            }
        
        total_chunks = 0
        processed_files = 0
        failed_files = []
        
        print(f"ğŸ“‚ {len(pdf_files)} PDF dosyasÄ± bulundu")
        print("ğŸ”„ Ä°ndeksleme baÅŸlÄ±yor...")
        
        for file_path in pdf_files:
            try:
                chunks_added = self.index_single_file(str(file_path), chunk_size, overlap)
                if chunks_added > 0:
                    total_chunks += chunks_added
                    processed_files += 1
                else:
                    failed_files.append(str(file_path))
            except Exception as e:
                print(f"âŒ {file_path.name}: {e}")
                failed_files.append(str(file_path))
        
        # SonuÃ§ raporu
        result = {
            'total_files': len(pdf_files),
            'processed_files': processed_files,
            'total_chunks': total_chunks,
            'failed_files': failed_files
        }
        
        print("\nğŸ“Š Ä°ndeksleme Raporu:")
        print(f"   ğŸ“ Toplam dosya: {result['total_files']}")
        print(f"   âœ… Ä°ÅŸlenen dosya: {result['processed_files']}")
        print(f"   ğŸ“„ Toplam chunk: {result['total_chunks']}")
        print(f"   âŒ BaÅŸarÄ±sÄ±z: {len(result['failed_files'])}")
        
        if result['failed_files']:
            print("\nâš ï¸  BaÅŸarÄ±sÄ±z dosyalar:")
            for file in result['failed_files']:
                print(f"   - {Path(file).name}")
        
        return result
    
    def get_database_stats(self) -> Dict:
        """VektÃ¶r veritabanÄ± istatistiklerini al"""
        return self.vector_db.get_collection_stats()
    
    def search_documents(self, query: str, n_results: int = 5) -> List[Dict]:
        """DokÃ¼manlarda arama yap"""
        return self.vector_db.search_similar(query, n_results)
    
    def clear_database(self) -> bool:
        """VeritabanÄ±nÄ± temizle"""
        return self.vector_db.clear_collection()

def main():
    """Standalone indeksleme scripti"""
    import argparse
    
    parser = argparse.ArgumentParser(description="PDF dosyalarÄ±nÄ± vektÃ¶r veritabanÄ±na indeksle")
    parser.add_argument("--directory", "-d", type=str, help="Ä°ndekslenecek klasÃ¶r")
    parser.add_argument("--file", "-f", type=str, help="Ä°ndekslenecek tek dosya")
    parser.add_argument("--chunk-size", type=int, default=1000, help="Chunk boyutu")
    parser.add_argument("--overlap", type=int, default=200, help="Ã–rtÃ¼ÅŸme miktarÄ±")
    parser.add_argument("--clear", action="store_true", help="VeritabanÄ±nÄ± temizle")
    parser.add_argument("--stats", action="store_true", help="VeritabanÄ± istatistiklerini gÃ¶ster")
    
    args = parser.parse_args()
    
    # API key kontrolÃ¼
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        print("âŒ GEMINI_API_KEY Ã§evre deÄŸiÅŸkeni bulunamadÄ±")
        return
    
    indexer = DocumentIndexer(api_key=api_key)
    
    if args.clear:
        print("ğŸ—‘ï¸  VeritabanÄ± temizleniyor...")
        if indexer.clear_database():
            print("âœ… VeritabanÄ± temizlendi")
        else:
            print("âŒ Temizleme baÅŸarÄ±sÄ±z")
        return
    
    if args.stats:
        stats = indexer.get_database_stats()
        print("ğŸ“Š VeritabanÄ± Ä°statistikleri:")
        print(f"   ğŸ“„ Dokuman sayÄ±sÄ±: {stats.get('document_count', 0)}")
        print(f"   ğŸ“ Collection: {stats.get('collection_name', 'N/A')}")
        print(f"   ğŸ’¾ VeritabanÄ± yolu: {stats.get('db_path', 'N/A')}")
        return
    
    if args.file:
        print(f"ğŸ“„ Tek dosya indeksleniyor: {args.file}")
        chunks_added = indexer.index_single_file(args.file, args.chunk_size, args.overlap)
        print(f"âœ… {chunks_added} chunk eklendi")
    
    elif args.directory:
        print(f"ğŸ“‚ KlasÃ¶r indeksleniyor: {args.directory}")
        result = indexer.index_directory(args.directory, args.chunk_size, args.overlap)
        print("âœ… Ä°ndeksleme tamamlandÄ±")
    
    else:
        print("âŒ --directory veya --file parametresi gerekli")
        parser.print_help()

if __name__ == "__main__":
    main()